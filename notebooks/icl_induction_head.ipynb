{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4cb8GADCuFl_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Apr  1 16:14:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.146.02             Driver Version: 535.146.02   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA RTX A6000               On  | 00000000:21:00.0 Off |                  Off |\n",
            "| 51%   76C    P2             114W / 200W |   3020MiB / 49140MiB |     13%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0 Off |                  Off |\n",
            "| 30%   57C    P8              20W / 200W |   9975MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA RTX A6000               On  | 00000000:43:00.0 Off |                  Off |\n",
            "| 30%   53C    P8              24W / 200W |   9960MiB / 49140MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzF5h6iNugUD"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pcrkWn3UuPgm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import einops\n",
        "\n",
        "\n",
        "class Unembed(nn.Module):\n",
        "    def __init__(self, d_vocab, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_vocab, d_model) / np.sqrt(d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.einsum(\"pe,bse->bsp\", self.W, x)\n",
        "\n",
        "\n",
        "class HookPoint(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fwd_hooks = []\n",
        "        self.bwd_hooks = []\n",
        "\n",
        "    def give_name(self, name):\n",
        "        # Called by the model at initialisation\n",
        "        self.name = name\n",
        "\n",
        "    def add_hook(self, hook, dir=\"fwd\"):\n",
        "        # Hook format is fn(activation, hook_name)\n",
        "        # Change it into PyTorch hook format (this includes input and output,\n",
        "        # which are the same for a HookPoint)\n",
        "        def full_hook(module, module_input, module_output):\n",
        "            return hook(module_output, name=self.name)\n",
        "\n",
        "        if dir == \"fwd\":\n",
        "            handle = self.register_forward_hook(full_hook)\n",
        "            self.fwd_hooks.append(handle)\n",
        "        elif dir == \"bwd\":\n",
        "            handle = self.register_backward_hook(full_hook)\n",
        "            self.bwd_hooks.append(handle)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def remove_hooks(self, dir=\"fwd\"):\n",
        "        if (dir == \"fwd\") or (dir == \"both\"):\n",
        "            for hook in self.fwd_hooks:\n",
        "                hook.remove()\n",
        "            self.fwd_hooks = []\n",
        "        if (dir == \"bwd\") or (dir == \"both\"):\n",
        "            for hook in self.bwd_hooks:\n",
        "                hook.remove()\n",
        "            self.bwd_hooks = []\n",
        "        if dir not in [\"fwd\", \"bwd\", \"both\"]:\n",
        "            raise ValueError(f\"Invalid direction {dir}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class PosEmbed(nn.Module):\n",
        "    def __init__(self, max_ctx, d_model, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model) * weight_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.W_pos[: x.shape[-2]]\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, epsilon=1e-4, model=[None]):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
        "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.model[0].use_ln:\n",
        "            x = x - x.mean(axis=-1)[..., None]\n",
        "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
        "            x = x * self.w_ln\n",
        "            x = x + self.b_ln\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (113 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    n_ctx : token size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_head, n_ctx):\n",
        "        super().__init__()\n",
        "        self.W_K = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_Q = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_V = nn.Parameter(\n",
        "            torch.randn(num_heads, d_head, d_model) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.W_O = nn.Parameter(\n",
        "            torch.randn(d_model, d_head * num_heads) / np.sqrt(d_model)\n",
        "        )\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones((n_ctx, n_ctx))))\n",
        "        self.d_head = d_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        k = torch.einsum(\"ihd,bpd->biph\", self.W_K, x)\n",
        "        q = torch.einsum(\"ihd,bpd->biph\", self.W_Q, x)\n",
        "        v = torch.einsum(\"ihd,bpd->biph\", self.W_V, x)\n",
        "        attn_scores_pre = torch.einsum(\"biph,biqh->biqp\", k, q)\n",
        "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (\n",
        "            1 - self.mask[: x.shape[-2], : x.shape[-2]]\n",
        "        )\n",
        "        attn_matrix = F.softmax(attn_scores_masked / np.sqrt(self.d_head), dim=-1)\n",
        "        z = torch.einsum(\"biph,biqp->biqh\", v, attn_matrix)\n",
        "        z_flat = einops.rearrange(z, \"b i q h -> b q (i h)\")\n",
        "        out = torch.einsum(\"df,bqf->bqd\", self.W_O, z_flat)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, d_in, d_out, act_type, weight_scale=1):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.randn(d_out, d_in))\n",
        "        torch.nn.init.normal_(self.W, mean=0, std=weight_scale / np.sqrt(d_in))\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * weight_ratio)\n",
        "\n",
        "    def set_weight_ratio_l2(self, weight_ratio):\n",
        "        self.W = nn.Parameter(self.W * torch.sqrt(weight_ratio))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.W.T\n",
        "\n",
        "\n",
        "# for Transformer\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size (114 or 3)\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, act_type):\n",
        "        super().__init__()\n",
        "        # bias & layer norm are removed.\n",
        "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model) / np.sqrt(d_model))\n",
        "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
        "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp) / np.sqrt(d_model))\n",
        "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
        "        self.act_type = act_type\n",
        "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
        "        assert act_type in [\"ReLU\", \"GeLU\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.einsum(\"md,bpd->bpm\", self.W_in, x) + self.b_in\n",
        "        if self.act_type == \"ReLU\":\n",
        "            x = F.relu(x)\n",
        "        elif self.act_type == \"GeLU\":\n",
        "            x = F.gelu(x)\n",
        "        x = torch.einsum(\"dm,bpm->bpd\", self.W_out, x) + self.b_out\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.W_in = nn.Parameter(self.W_in * weight_ratio)\n",
        "        self.W_out = nn.Parameter(self.W_out * weight_ratio)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    b : batch size\n",
        "    d : embedding size of token\n",
        "    p : vocabraly size\n",
        "    i : number of heads\n",
        "    h : embedding size of each heads\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
        "        super().__init__()\n",
        "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
        "        self.model = model\n",
        "        self.attn = Attention(d_model, num_heads, d_head, n_ctx)\n",
        "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
        "        self.mlp = MLPBlock(d_model, d_mlp, act_type)\n",
        "        self.layer_norm = LayerNorm(d_model, model=self.model)\n",
        "        self.hook_attn_out = HookPoint()\n",
        "        self.hook_mlp_out = HookPoint()\n",
        "        self.hook_resid_pre = HookPoint()\n",
        "        self.hook_resid_mid = HookPoint()\n",
        "        self.hook_resid_post = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hook_resid_mid(\n",
        "            x + self.hook_attn_out(self.attn((self.hook_resid_pre(x))))\n",
        "        )\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
        "        return x\n",
        "\n",
        "    def set_weight_ratio(self, weight_ratio):\n",
        "        self.attn.set_weight_ratio(weight_ratio)\n",
        "        self.mlp.set_weight_ratio(weight_ratio)\n",
        "\n",
        "\n",
        "class InputEmbedder(nn.Module):\n",
        "    \"\"\"Input embedder.\"\"\"\n",
        "\n",
        "    def __init__(self, conf):\n",
        "\n",
        "        \"\"\"Initialize the input embedder.\n",
        "\n",
        "    Args:\n",
        "      num_classes: Total number of output classes.\n",
        "      emb_dim: Dimensionality of example and label embeddings.\n",
        "      example_encoding: How to encode example inputs.\n",
        "        'resnet': simple resnet encoding\n",
        "        'linear': flatten and pass through a linear layer\n",
        "        'embedding': pass through an embedding layer\n",
        "      flatten_superpixels: Whether to flatten the output of the resnet (instead\n",
        "        of taking a mean over superpixels).\n",
        "      example_dropout_prob: Dropout probability on example embeddings. Note that\n",
        "        these are applied at both train and test.\n",
        "      concatenate_labels: Whether to concatenate example and label embeddings\n",
        "        into one token for each (example, label) pair, rather than being fed to\n",
        "        the transformer as two separate tokens.\n",
        "      use_positional_encodings: Whether to use positional encoding.\n",
        "      positional_dropout_prob: Positional dropout probability.\n",
        "      name: Optional name for the module.\n",
        "    \"\"\"\n",
        "        super(InputEmbedder, self).__init__()\n",
        "        self.num_labels = conf.d_vocab\n",
        "        self.emb_dim = conf.d_emb\n",
        "        self.p_dim = conf.p_dim\n",
        "        self.emb_dim_content = self.emb_dim - self.p_dim\n",
        "        self.n_ctx = conf.n_ctx\n",
        "\n",
        "        self.Emb = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.label_embs = nn.Parameter(\n",
        "            torch.randn(self.num_labels, self.emb_dim_content) / np.sqrt(self.emb_dim_content)\n",
        "        )\n",
        "\n",
        "    def forward(self, examples, labels, is_training=True):\n",
        "        \"\"\"Call to the input embedder.\n",
        "\n",
        "        Args:\n",
        "          examples: input sequence of shape\n",
        "            [batch_size, seq_len, height, width, channels]\n",
        "          labels: input sequence of shape [batch_size, seq_len]\n",
        "          is_training: if is currently training.\n",
        "\n",
        "        Returns:\n",
        "          outputs: output of the transformer tower\n",
        "            of shape [batch_size, seq_len, channels].\n",
        "        \"\"\"\n",
        "        # Encode the example inputs into shape (B, SS, E)\n",
        "        B, SS, D = examples.shape\n",
        "        # pos encoding\n",
        "        pos_enc = F.one_hot(torch.arange(start=0,end=self.n_ctx+1,step=2), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
        "        h_example = torch.cat([examples, pos_enc], dim=2)\n",
        "\n",
        "        # Embed the labels.\n",
        "        labels_to_embed = labels\n",
        "        h_label = self.label_embs[labels_to_embed]  # (B, SS, D)\n",
        "        pos_enc = F.one_hot(torch.arange(start=1,end=self.n_ctx+1,step=2), num_classes=self.p_dim).repeat(B,1,1).to(examples.device)\n",
        "        h_label = torch.cat([h_label, pos_enc], dim=2) # (B, SS, E)\n",
        "        \n",
        "        hh = torch.empty(\n",
        "            (h_example.shape[0], h_example.shape[1] * 2 - 1, h_example.shape[2]),\n",
        "            dtype=h_example.dtype,\n",
        "        ).to(h_example.device)\n",
        "        \n",
        "        hh[:, 0::2] = h_example\n",
        "        hh[:, 1::2] = h_label[:, :-1]\n",
        "\n",
        "        return hh\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                TransformerBlock(\n",
        "                    d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]\n",
        "                )\n",
        "                for i in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels,)\n",
        "        # x = self.pos_embed(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")\n",
        "\n",
        "class TransformerICL(nn.Module):\n",
        "    def __init__(self, embedder, config):\n",
        "        super().__init__()\n",
        "        num_layers = config.num_layers\n",
        "        d_model = config.d_emb\n",
        "        d_mlp = config.d_emb * 4\n",
        "        d_head = config.d_emb // config.num_heads\n",
        "        num_heads = config.num_heads\n",
        "        n_ctx = config.n_ctx\n",
        "        act_type = config.act_type\n",
        "        use_cache = config.use_cache\n",
        "        use_ln = config.use_ln\n",
        "        self.cache = {}\n",
        "        self.use_cache = use_cache\n",
        "        d_vocab = config.d_vocab\n",
        "\n",
        "        self.embedder = embedder\n",
        "        # self.pos_embed = PosEmbed(n_ctx, d_model)\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Attention(d_model, num_heads, d_head, n_ctx),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "                Dense(d_model, d_model, act_type),\n",
        "            ]\n",
        "        )\n",
        "        # self.ln = LayerNorm(d_model, model=[self])\n",
        "        self.unembed = Unembed(d_vocab, d_model)\n",
        "        self.use_ln = use_ln\n",
        "\n",
        "        for name, module in self.named_modules():\n",
        "            if type(module) == HookPoint:\n",
        "                module.give_name(name)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        x = self.embedder(x, labels)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.unembed(x)\n",
        "        return x\n",
        "\n",
        "    def set_use_cache(self, use_cache):\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "    def hook_points(self):\n",
        "        return [module for name, module in self.named_modules() if \"hook\" in name]\n",
        "\n",
        "    def remove_all_hooks(self):\n",
        "        for hp in self.hook_points():\n",
        "            hp.remove_hooks(\"fwd\")\n",
        "            hp.remove_hooks(\"bwd\")\n",
        "\n",
        "    def cache_all(self, cache, incl_bwd=False):\n",
        "        # Caches all activations wrapped in a HookPoint\n",
        "        def save_hook(tensor, name):\n",
        "            cache[name] = tensor.detach()\n",
        "\n",
        "        def save_hook_back(tensor, name):\n",
        "            cache[name + \"_grad\"] = tensor[0].detach()\n",
        "\n",
        "        for hp in self.hook_points():\n",
        "            hp.add_hook(save_hook, \"fwd\")\n",
        "            if incl_bwd:\n",
        "                hp.add_hook(save_hook_back, \"bwd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6iNOyJRwwV1h"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class SamplingDataset(object):\n",
        "  def __init__(self,conf):\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.dim = conf.dim\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.mu, self.labels = self._get_data()\n",
        "\n",
        "  def _get_data(self):\n",
        "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(self.num_classes,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(self.num_classes,1))\n",
        "    return mu, labels\n",
        "\n",
        "class SamplingLoader(DataLoader):\n",
        "\n",
        "  def __init__(self,conf, dataset):\n",
        "    self.dataset = dataset\n",
        "    self.mu, self.labels = self.dataset.mu, self.dataset.labels\n",
        "    self.data_type = conf.data_type\n",
        "    self.num_seq = conf.num_seq\n",
        "    self.alpha = conf.alpha\n",
        "    self.num_classes = conf.num_classes\n",
        "    self.num_labels = conf.num_labels\n",
        "    self.ways = conf.ways\n",
        "    self.p_bursty = conf.p_bursty\n",
        "    self.p_icl = conf.p_icl\n",
        "    self.eps = conf.eps\n",
        "    self.dim = conf.dim\n",
        "    if self.ways != 0:\n",
        "      assert self.num_seq % self.ways == 0\n",
        "    if self.ways == 0:\n",
        "      self.p_bursty = 0\n",
        "    prob = np.array([1/((k+1)**self.alpha) for k in range(self.num_classes)])\n",
        "    self.prob = prob/prob.sum()\n",
        "\n",
        "  def get_seq(self):\n",
        "    while True:\n",
        "      if self.data_type==\"bursty\":\n",
        "        if self.p_icl > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            mus, labels = self._get_novel_class_seq(num_few_shot_class)\n",
        "            # mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            # labels = self.labels[few_shot_class]\n",
        "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "            classes = np.arange(num_few_shot_class)\n",
        "            classes = np.repeat(classes, self.ways)\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            mus = mus[ordering]\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class_idx = np.random.choice(len(classes), 1)\n",
        "            query_class = classes[query_class_idx]\n",
        "            query_label = labels[query_class_idx]\n",
        "            query_mu = mus[query_class_idx]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            \n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "            \n",
        "        else:\n",
        "          if self.p_bursty > np.random.rand():\n",
        "            # choise few shot example\n",
        "            num_few_shot_class = self.num_seq//self.ways\n",
        "            few_shot_class = np.random.choice(self.num_classes, num_few_shot_class, replace=False)\n",
        "            mus = self.mu[few_shot_class]\n",
        "            mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "            labels = self.labels[few_shot_class]\n",
        "            labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "            classes = np.repeat(few_shot_class, self.ways)\n",
        "            # add noise\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "            # select query labels\n",
        "            query_class = np.random.choice(few_shot_class, 1)\n",
        "            query_label = self.labels[query_class]\n",
        "            query_mu = self.mu[query_class]\n",
        "            query_x = self.add_noise(query_mu)\n",
        "            # concat\n",
        "            x = torch.cat([x, query_x])\n",
        "            labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels,\n",
        "                \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "            }\n",
        "          else:\n",
        "            # rank frequency\n",
        "            classes = np.random.choice(self.num_classes, self.num_seq+1, p=self.prob)\n",
        "            mus = self.mu[classes]\n",
        "            labels = self.labels[classes]\n",
        "            x = self.add_noise(mus)\n",
        "            # permutation shuffle\n",
        "            ordering = np.random.permutation(self.num_seq+1)\n",
        "            x = x[ordering]\n",
        "            labels = labels[ordering]\n",
        "            classes = classes[ordering]\n",
        "\n",
        "            yield {\n",
        "                \"examples\":x.to(torch.float32),\n",
        "                \"labels\":labels.flatten(),\n",
        "                \"classes\" : torch.from_numpy(classes)\n",
        "            }\n",
        "\n",
        "      elif self.data_type == \"no_support\":\n",
        "          # rank frequency\n",
        "          classes = np.random.choice(self.num_classes, self.num_seq+1, p=self.prob)\n",
        "          mus = self.mu[classes]\n",
        "          labels = self.labels[classes]\n",
        "          x = self.add_noise(mus)\n",
        "          # permutation shuffle\n",
        "          ordering = np.random.permutation(self.num_seq+1)\n",
        "          x = x[ordering]\n",
        "          labels = labels[ordering]\n",
        "          classes = classes[ordering]\n",
        "\n",
        "          yield {\n",
        "              \"examples\":x.to(torch.float32),\n",
        "              \"labels\":labels.flatten(),\n",
        "              \"classes\" : torch.from_numpy(classes)\n",
        "          }\n",
        "          \n",
        "      elif self.data_type == \"holdout\":\n",
        "        # choise few shot example\n",
        "        num_few_shot_class = self.num_seq//self.ways\n",
        "        mus, labels = self._get_novel_class_seq(num_few_shot_class)\n",
        "        # mus = self.mu[few_shot_class]\n",
        "        mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "        # labels = self.labels[few_shot_class]\n",
        "        labels = np.repeat(labels, self.ways, axis=0) # expand ways\n",
        "        classes = np.arange(num_few_shot_class)\n",
        "        classes = np.repeat(classes, self.ways)\n",
        "        # add noise\n",
        "        x = self.add_noise(mus)\n",
        "        # permutation shuffle\n",
        "        ordering = np.random.permutation(self.num_seq)\n",
        "        x = x[ordering]\n",
        "        mus = mus[ordering]\n",
        "        labels = labels[ordering]\n",
        "        classes = classes[ordering]\n",
        "        # select query labels\n",
        "        query_class_idx = np.random.choice(len(classes), 1)\n",
        "        query_class = classes[query_class_idx]\n",
        "        query_label = labels[query_class_idx]\n",
        "        query_mu = mus[query_class_idx]\n",
        "        query_x = self.add_noise(query_mu)\n",
        "        # concat\n",
        "        x = torch.cat([x, query_x])\n",
        "        labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "        \n",
        "        yield {\n",
        "            \"examples\":x.to(torch.float32),\n",
        "            \"labels\":labels,\n",
        "            \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "        }\n",
        "\n",
        "      elif self.data_type == \"flip\":\n",
        "        # choise few shot example\n",
        "        num_few_shot_class = self.num_seq//self.ways\n",
        "        few_shot_class = np.random.choice(self.num_classes, num_few_shot_class, replace=False)\n",
        "        mus = self.mu[few_shot_class]\n",
        "        mus = np.repeat(mus, self.ways, axis=0) # expand ways\n",
        "        classes = np.repeat(few_shot_class, self.ways)\n",
        "        # label flip\n",
        "        labels = (self.labels[classes] + 1) % self.num_labels\n",
        "        # add noise\n",
        "        x = self.add_noise(mus)\n",
        "        # permutation shuffle\n",
        "        ordering = np.random.permutation(self.num_seq)\n",
        "        x = x[ordering]\n",
        "        labels = labels[ordering]\n",
        "        classes = classes[ordering]\n",
        "        # select query labels\n",
        "        query_class = np.random.choice(few_shot_class, 1)\n",
        "        query_label = (self.labels[query_class] + 1) % self.num_labels\n",
        "        query_mu = self.mu[query_class]\n",
        "        query_x = self.add_noise(query_mu)\n",
        "        # concat\n",
        "        x = torch.cat([x, query_x])\n",
        "        labels = torch.cat([labels.flatten(), query_label.flatten()])\n",
        "        \n",
        "        yield {\n",
        "            \"examples\":x.to(torch.float32),\n",
        "            \"labels\":labels,\n",
        "            \"classes\" : torch.cat([torch.from_numpy(classes).flatten(), torch.from_numpy(query_class).flatten()])\n",
        "        }\n",
        "    \n",
        "  \n",
        "\n",
        "  def add_noise(self, x):\n",
        "    x = (x+self.eps*torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(x.shape)))/(np.sqrt(1+self.eps**2))\n",
        "    # x = (x+self.eps*np.random.normal(mean=0, std=np.sqrt(1/self.dim), size=(x.shape[0],1)))/(np.sqrt(1+self.eps**2))\n",
        "    return x\n",
        "  \n",
        "  def _get_novel_class_seq(self,num_class):\n",
        "    mu = torch.normal(mean=0, std=math.sqrt(1/self.dim), size=(num_class,self.dim))\n",
        "    labels = torch.randint(self.num_labels, size=(num_class,1))\n",
        "    return mu, labels\n",
        "\n",
        "class IterDataset(IterableDataset):\n",
        "    def __init__(self, generator):\n",
        "        self.generator = generator\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.generator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gX3jnutBzfVe"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "\n",
        "@dataclass\n",
        "class TransformerConfig:\n",
        "  num_layers: int = 2\n",
        "  d_vocab: int = 32 # same as num_labels\n",
        "  d_model: int = 128 \n",
        "  d_mlp: int = 128\n",
        "  d_head: int = 128\n",
        "  num_heads: int = 1\n",
        "  n_ctx: int = int(8*2+1)\n",
        "  act_type: str = \"ReLU\"\n",
        "  use_cache: bool = False\n",
        "  use_ln: bool = True\n",
        "  p_dim: int = 65\n",
        "  d_emb: int = 128\n",
        "\n",
        "@dataclass\n",
        "class TrainDataConfig:\n",
        "  num_classes: int = 512\n",
        "  dim: int = 63\n",
        "  num_labels: int = 32\n",
        "  eps: float = 0.1\n",
        "  alpha: float = 0 \n",
        "  ways: int = 2 # Birstiness\n",
        "  num_seq: int = 8\n",
        "  p_bursty: float =  1\n",
        "  p_icl: float = 0\n",
        "  data_type: str = \"bursty\" # bursty, holdout, no_support, flip\n",
        "\n",
        "@dataclass\n",
        "class IWLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"no_support\" # bursty, holdout, no_support, flip\n",
        "\n",
        "@dataclass\n",
        "class ICLDataConfig(TrainDataConfig):\n",
        "  data_type: str = \"holdout\" # bursty, holdout, no_support, flip\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ICL2DataConfig(TrainDataConfig):\n",
        "  data_type: str = \"flip\" # bursty, holdout, no_support, flip\n",
        "  \n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "  batch_size: int = 128\n",
        "  optimize_step: int = int(2e5)\n",
        "  lr: float = 0.01\n",
        "  optimizer: str = \"sgd\" # adam, sgd, adamw\n",
        "\n",
        "@dataclass\n",
        "class MainConfig:\n",
        "  traindataconfig : TrainDataConfig = TrainDataConfig()\n",
        "  icldataconfig: ICLDataConfig = ICLDataConfig()\n",
        "  iwldataconfig: IWLDataConfig = IWLDataConfig()\n",
        "  icl2dataconfig: ICL2DataConfig = ICL2DataConfig()\n",
        "  modelconfig: TransformerConfig = TransformerConfig()\n",
        "  trainconfig: TrainConfig = TrainConfig()\n",
        "  device: str = \"cuda:1\"\n",
        "# define config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "U4wKEdZkNkC6"
      },
      "outputs": [],
      "source": [
        "def cal_acc(t,p):\n",
        "    p_arg = torch.argmax(p,dim=1)\n",
        "    return torch.sum(t == p_arg) / p.shape[0]\n",
        "def to_gpu_dict(dic):\n",
        "    dic = {k:v.to(\"cuda:1\") for k,v in dic.items()}\n",
        "    return dic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_class tensor([[206, 206, 463,  ..., 469,  64, 206],\n",
            "        [ 84,  47, 278,  ...,  47, 278, 278],\n",
            "        [232, 474,  49,  ...,  99,  99,  99],\n",
            "        ...,\n",
            "        [300, 243, 243,  ..., 224, 224, 300],\n",
            "        [129, 129,  57,  ...,  57, 445, 445],\n",
            "        [ 81, 454, 425,  ..., 121, 454, 425]])\n",
            "train_label tensor([[27, 27, 17,  ..., 18, 12, 27],\n",
            "        [ 2,  3, 20,  ...,  3, 20, 20],\n",
            "        [30, 10, 20,  ...,  4,  4,  4],\n",
            "        ...,\n",
            "        [24, 24, 24,  ...,  1,  1, 24],\n",
            "        [10, 10, 25,  ..., 25, 24, 24],\n",
            "        [ 7,  2,  9,  ...,  7,  2,  9]])\n",
            "train_class tensor([[294, 294,  75,  ..., 227,  75,  75],\n",
            "        [482, 163, 153,  ..., 153, 280, 153],\n",
            "        [457,  87, 457,  ..., 316,  75, 316],\n",
            "        ...,\n",
            "        [463, 463,  71,  ...,  71,  77,  77],\n",
            "        [376, 189, 376,  ..., 244,  53, 376],\n",
            "        [ 84, 502,  84,  ..., 465, 182, 182]])\n",
            "train_label tensor([[26, 26, 27,  ..., 14, 27, 27],\n",
            "        [18, 29, 10,  ..., 10, 29, 10],\n",
            "        [31, 29, 31,  ..., 19, 27, 19],\n",
            "        ...,\n",
            "        [17, 17, 13,  ..., 13, 22, 22],\n",
            "        [31, 11, 31,  ..., 22, 26, 31],\n",
            "        [ 2, 31,  2,  ..., 13,  6,  6]])\n",
            "train_class tensor([[ 89, 274,  75,  ...,  89, 274,  75],\n",
            "        [390,  95,  95,  ..., 390, 304, 304],\n",
            "        [409, 476,  84,  ..., 409, 349, 476],\n",
            "        ...,\n",
            "        [215, 371, 444,  ..., 444, 371, 444],\n",
            "        [221, 263, 189,  ..., 227, 263, 189],\n",
            "        [177, 177, 311,  ..., 440, 311, 440]])\n",
            "train_label tensor([[10,  1, 27,  ..., 10,  1, 27],\n",
            "        [ 2, 26, 26,  ...,  2, 23, 23],\n",
            "        [ 0,  5,  2,  ...,  0,  2,  5],\n",
            "        ...,\n",
            "        [27, 16, 24,  ..., 24, 16, 24],\n",
            "        [ 9, 17, 11,  ..., 14, 17, 11],\n",
            "        [20, 20, 24,  ..., 23, 24, 23]])\n",
            "iwl_class tensor([[472,  94, 284,  ...,  28, 374, 490],\n",
            "        [469, 285,  94,  ..., 145, 186, 210],\n",
            "        [ 61, 221, 177,  ..., 385, 484, 354],\n",
            "        ...,\n",
            "        [273,  92, 160,  ..., 491, 388, 163],\n",
            "        [317, 400, 305,  ..., 212, 394, 382],\n",
            "        [ 51, 446, 406,  ..., 336, 395,  87]])\n",
            "iwl_label tensor([[24,  3, 27,  ..., 25, 22,  9],\n",
            "        [18, 27,  3,  ..., 31, 29, 20],\n",
            "        [31,  9, 20,  ..., 22, 13, 21],\n",
            "        ...,\n",
            "        [23, 24, 29,  ...,  4, 25, 29],\n",
            "        [21, 21, 25,  ...,  4, 14, 27],\n",
            "        [ 2,  6, 25,  ..., 24,  9, 29]])\n",
            "iwl_class tensor([[440, 301, 312,  ..., 208, 261, 107],\n",
            "        [284, 472, 246,  ...,  76, 397, 241],\n",
            "        [315, 377, 511,  ..., 346,   2,  21],\n",
            "        ...,\n",
            "        [394, 496, 308,  ..., 362, 442, 270],\n",
            "        [157, 384, 492,  ...,  85, 383, 243],\n",
            "        [435, 270, 443,  ...,  67, 279, 265]])\n",
            "iwl_label tensor([[23, 11, 27,  ..., 27, 25,  4],\n",
            "        [27, 24, 20,  ..., 26, 20,  1],\n",
            "        [14,  1,  9,  ...,  2, 17,  1],\n",
            "        ...,\n",
            "        [14,  7, 23,  ...,  0, 14,  1],\n",
            "        [ 0, 28,  1,  ...,  3, 18, 24],\n",
            "        [17,  1,  4,  ...,  9, 11,  0]])\n",
            "iwl_class tensor([[182, 415, 274,  ..., 276,  62, 124],\n",
            "        [ 66, 406,  29,  ...,  77, 232,  82],\n",
            "        [463, 431, 402,  ...,   8, 234, 309],\n",
            "        ...,\n",
            "        [375, 471,  33,  ..., 345, 219, 293],\n",
            "        [ 64, 500, 398,  ..., 312, 116, 238],\n",
            "        [172, 470, 375,  ..., 228,  16, 191]])\n",
            "iwl_label tensor([[ 6, 21,  1,  ..., 10, 13,  3],\n",
            "        [14, 25,  2,  ..., 22, 30, 31],\n",
            "        [17, 17, 26,  ..., 24, 22, 16],\n",
            "        ...,\n",
            "        [ 1,  8, 30,  ...,  9,  1,  8],\n",
            "        [12,  5,  6,  ..., 27,  3, 13],\n",
            "        [24,  3,  1,  ..., 10, 29, 27]])\n",
            "icl2_class tensor([[ 22, 117, 457,  ..., 319, 117, 319],\n",
            "        [ 55, 162, 446,  ..., 162,  46, 446],\n",
            "        [431, 124, 204,  ..., 204,  59, 124],\n",
            "        ...,\n",
            "        [337, 337, 116,  ..., 116, 112, 112],\n",
            "        [221, 158, 158,  ..., 239, 239,   3],\n",
            "        [ 15,  17, 234,  ...,  17, 274,  15]])\n",
            "icl2_label tensor([[ 1, 20,  0,  ...,  8, 20,  8],\n",
            "        [ 4,  4,  7,  ...,  4, 18,  7],\n",
            "        [18,  4, 11,  ..., 11,  3,  4],\n",
            "        ...,\n",
            "        [29, 29,  4,  ...,  4, 23, 23],\n",
            "        [10, 13, 13,  ..., 31, 31, 26],\n",
            "        [17,  7, 23,  ...,  7,  2, 17]])\n",
            "icl2_class tensor([[295, 295, 339,  ..., 142, 258, 258],\n",
            "        [235, 301, 143,  ..., 301, 143, 131],\n",
            "        [504, 498, 504,  ...,  62,  62,  62],\n",
            "        ...,\n",
            "        [311,  64,  31,  ...,  56,  56,  64],\n",
            "        [248, 248,  96,  ..., 470, 312, 248],\n",
            "        [186, 405, 387,  ..., 170, 387, 186]])\n",
            "icl2_label tensor([[20, 20, 18,  ...,  8, 25, 25],\n",
            "        [28, 12,  5,  ..., 12,  5, 20],\n",
            "        [16, 31, 16,  ..., 14, 14, 14],\n",
            "        ...,\n",
            "        [25, 13, 21,  ..., 21, 21, 13],\n",
            "        [19, 19, 11,  ...,  4, 28, 19],\n",
            "        [30, 22,  7,  ..., 10,  7, 30]])\n",
            "icl2_class tensor([[457, 221, 242,  ..., 457, 221, 242],\n",
            "        [356, 116, 356,  ..., 325, 325, 116],\n",
            "        [235, 246, 235,  ..., 411, 246, 411],\n",
            "        ...,\n",
            "        [346, 465, 108,  ...,  19, 346, 346],\n",
            "        [170,  42, 210,  ..., 347,  42,  42],\n",
            "        [422, 184, 184,  ..., 422, 313, 184]])\n",
            "icl2_label tensor([[ 0, 10, 28,  ...,  0, 10, 28],\n",
            "        [25,  4, 25,  ...,  0,  0,  4],\n",
            "        [28, 21, 28,  ..., 13, 21, 13],\n",
            "        ...,\n",
            "        [ 3, 14, 24,  ..., 22,  3,  3],\n",
            "        [10, 18, 21,  ..., 16, 18, 18],\n",
            "        [ 2, 28, 28,  ...,  2,  9, 28]])\n",
            "icl_class tensor([[2, 0, 0,  ..., 1, 3, 3],\n",
            "        [2, 1, 3,  ..., 0, 0, 1],\n",
            "        [2, 0, 3,  ..., 2, 1, 2],\n",
            "        ...,\n",
            "        [2, 3, 3,  ..., 0, 1, 0],\n",
            "        [2, 0, 3,  ..., 2, 3, 3],\n",
            "        [1, 0, 1,  ..., 2, 3, 1]])\n",
            "icl_label tensor([[ 5, 10, 10,  ...,  5, 20, 20],\n",
            "        [ 8, 16, 23,  ..., 28, 28, 16],\n",
            "        [24,  8, 11,  ..., 24,  5, 24],\n",
            "        ...,\n",
            "        [ 7, 21, 21,  ..., 16, 27, 16],\n",
            "        [ 2, 16, 22,  ...,  2, 22, 22],\n",
            "        [ 5, 26,  5,  ..., 26,  0,  5]])\n",
            "icl_class tensor([[2, 2, 1,  ..., 0, 1, 0],\n",
            "        [2, 0, 1,  ..., 3, 0, 2],\n",
            "        [3, 1, 3,  ..., 0, 1, 3],\n",
            "        ...,\n",
            "        [2, 1, 3,  ..., 3, 1, 0],\n",
            "        [3, 3, 1,  ..., 0, 0, 1],\n",
            "        [2, 1, 0,  ..., 3, 0, 1]])\n",
            "icl_label tensor([[13, 13, 25,  ..., 16, 25, 16],\n",
            "        [13,  2, 20,  ..., 17,  2, 13],\n",
            "        [ 3, 29,  3,  ..., 20, 29,  3],\n",
            "        ...,\n",
            "        [25,  2, 14,  ..., 14,  2,  5],\n",
            "        [24, 24,  9,  ...,  3,  3,  9],\n",
            "        [29, 24, 10,  ...,  4, 10, 24]])\n",
            "icl_class tensor([[1, 3, 1,  ..., 2, 0, 3],\n",
            "        [1, 3, 1,  ..., 0, 0, 3],\n",
            "        [1, 2, 2,  ..., 1, 3, 0],\n",
            "        ...,\n",
            "        [0, 0, 3,  ..., 1, 2, 2],\n",
            "        [3, 0, 3,  ..., 0, 1, 0],\n",
            "        [3, 1, 2,  ..., 1, 2, 1]])\n",
            "icl_label tensor([[31, 14, 31,  ..., 11,  7, 14],\n",
            "        [ 1, 10,  1,  ..., 14, 14, 10],\n",
            "        [12, 22, 22,  ..., 12,  0,  4],\n",
            "        ...,\n",
            "        [10, 10, 22,  ...,  6,  7,  7],\n",
            "        [22, 16, 22,  ..., 16,  7, 16],\n",
            "        [16,  2, 13,  ...,  2, 13,  2]])\n",
            "icl_class tensor([[2, 0, 1,  ..., 1, 3, 3],\n",
            "        [2, 1, 0,  ..., 1, 0, 1],\n",
            "        [1, 2, 3,  ..., 1, 2, 2],\n",
            "        ...,\n",
            "        [2, 3, 3,  ..., 0, 2, 3],\n",
            "        [3, 0, 0,  ..., 1, 1, 3],\n",
            "        [3, 1, 0,  ..., 0, 1, 3]])\n",
            "icl_label tensor([[29, 18,  9,  ...,  9,  8,  8],\n",
            "        [28, 26,  5,  ..., 26,  5, 26],\n",
            "        [13,  1, 22,  ..., 13,  1,  1],\n",
            "        ...,\n",
            "        [ 7, 29, 29,  ..., 18,  7, 29],\n",
            "        [31, 13, 13,  ..., 27, 27, 31],\n",
            "        [11, 13,  4,  ...,  4, 13, 11]])\n",
            "icl_class tensor([[2, 0, 0,  ..., 3, 3, 0],\n",
            "        [1, 1, 0,  ..., 0, 3, 1],\n",
            "        [2, 3, 0,  ..., 2, 0, 0],\n",
            "        ...,\n",
            "        [2, 3, 0,  ..., 1, 3, 2],\n",
            "        [1, 0, 2,  ..., 3, 0, 1],\n",
            "        [3, 2, 3,  ..., 1, 2, 0]])\n",
            "icl_label tensor([[19, 17, 17,  ..., 25, 25, 17],\n",
            "        [16, 16,  3,  ...,  3, 18, 16],\n",
            "        [25, 25, 20,  ..., 25, 20, 20],\n",
            "        ...,\n",
            "        [12, 18, 23,  ...,  9, 18, 12],\n",
            "        [ 8,  1, 16,  ...,  5,  1,  8],\n",
            "        [25, 17, 25,  ..., 24, 17,  6]])\n",
            "icl_class tensor([[2, 2, 0,  ..., 1, 0, 2],\n",
            "        [2, 2, 3,  ..., 1, 0, 1],\n",
            "        [2, 0, 1,  ..., 3, 0, 3],\n",
            "        ...,\n",
            "        [2, 2, 3,  ..., 1, 0, 2],\n",
            "        [3, 3, 1,  ..., 0, 1, 3],\n",
            "        [0, 2, 1,  ..., 0, 3, 2]])\n",
            "icl_label tensor([[28, 28, 27,  ..., 10, 27, 28],\n",
            "        [19, 19, 30,  ..., 28, 12, 28],\n",
            "        [ 9, 23, 18,  ..., 24, 23, 24],\n",
            "        ...,\n",
            "        [20, 20, 24,  ..., 15, 15, 20],\n",
            "        [29, 29,  9,  ...,  8,  9, 29],\n",
            "        [26,  4,  1,  ..., 26, 11,  4]])\n"
          ]
        }
      ],
      "source": [
        "traindataconfig = MainConfig.traindataconfig\n",
        "icldataconfig = MainConfig.icldataconfig\n",
        "iwldataconfig = MainConfig.iwldataconfig\n",
        "icl2dataconfig = MainConfig.icl2dataconfig\n",
        "trainconfig = MainConfig.trainconfig\n",
        "\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "cnt = 0\n",
        "for data in train_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"train_class\", classes)\n",
        "    print(\"train_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "    \n",
        "cnt = 0\n",
        "for data in iwl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"iwl_class\", classes)\n",
        "    print(\"iwl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl2_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples)\n",
        "    print(\"icl2_class\", classes)\n",
        "    print(\"icl2_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 2:\n",
        "        break\n",
        "cnt = 0\n",
        "for data in icl_dataloader:\n",
        "    examples = data[\"examples\"]\n",
        "    labels = data[\"labels\"]\n",
        "    classes = data[\"classes\"]\n",
        "    # print(examples\n",
        "    print(\"icl_class\", classes)\n",
        "    print(\"icl_label\", labels)\n",
        "    cnt += 1\n",
        "    if cnt > 5:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "12621ac3883347eba74b077797cda2b9",
            "5b7930a79e344c358f1ca175e9be4fd1",
            "d4e89f0d98b44eb4b76e59caa06aea14",
            "494267ae30c44a3bb446eaee6dc43115",
            "1a6e230516ff4996bb34f8a4992e1a20",
            "065c4cf26dd24c0d8caaa7165e800ca7",
            "aea29e116b304946b7d98692deb2addb",
            "9b3c46b8ff12462b9c4d2f0fbf12c767"
          ]
        },
        "id": "jwic1RdJGRDT",
        "outputId": "4663b1f4-a8bd-4983-f03d-8d0b9ac74208"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:sndp3zpb) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>valid/icl2_acc</td><td></td></tr><tr><td>valid/icl_acc</td><td></td></tr><tr><td>valid/iwl_acc</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/acc</td><td>1.0</td></tr><tr><td>train/loss</td><td>2.63257</td></tr><tr><td>valid/icl2_acc</td><td>0.0</td></tr><tr><td>valid/icl_acc</td><td>0.0</td></tr><tr><td>valid/iwl_acc</td><td>0.0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">drawn-violet-2</strong> at: <a href='https://wandb.ai/gouki/icl-test/runs/sndp3zpb' target=\"_blank\">https://wandb.ai/gouki/icl-test/runs/sndp3zpb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240401_161824-sndp3zpb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:sndp3zpb). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.16.5 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/induction-head/notebooks/wandb/run-20240401_163027-hst3tn8u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gouki/icl-test/runs/hst3tn8u' target=\"_blank\">peachy-feather-3</a></strong> to <a href='https://wandb.ai/gouki/icl-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gouki/icl-test' target=\"_blank\">https://wandb.ai/gouki/icl-test</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gouki/icl-test/runs/hst3tn8u' target=\"_blank\">https://wandb.ai/gouki/icl-test/runs/hst3tn8u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 11827 0.2890625 0.0234375 0.296875 0.25781255.078125 0.0468750.0703125 0.0625 0.11718750.1640625 0.0390625 0.1015625 0.1328125 0.203125 0.046875 0.1640625 0.19531250.1796875 0.046875 0.2734375 0.2109375 0.1875 0.203125 0.1796875 0.0390625 0.2578125 0.265625 0.015625 0.1953125 0.27343750.359375 0.0234375 0.1953125 0.2890625 0.3515625 0.25781257081 0.3046875 0.03125 0.3125 0.265625 0.2578125 0.015625 0.265625 0.28125 0.25 0.0390625 0.265625 0.35156250.3046875 0.0234375 0.2734375 0.2656250.0390625 0.2265625 0.2421875 0.390625 0.359375 0.015625 0.2421875 0.29687511300 0.265625 0.0234375 0.25 0.30468750.0234375 0.3359375 0.328125 0.2421875 0.0078125 0.28125 0.2421875 0.2421875 0.1875"
          ]
        }
      ],
      "source": [
        "# train\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "config = MainConfig()\n",
        "wandb.init(project=\"icl-test\", config=asdict(config))\n",
        "# data\n",
        "Dataset = SamplingDataset(traindataconfig)\n",
        "\n",
        "trainloader = SamplingLoader(traindataconfig, dataset=Dataset)\n",
        "train_seq_generator = trainloader.get_seq\n",
        "train_dataset = IterDataset(train_seq_generator)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iclloader = SamplingLoader(icldataconfig, dataset=Dataset)\n",
        "icl_seq_generator = iclloader.get_seq\n",
        "icl_dataset = IterDataset(icl_seq_generator)\n",
        "icl_dataloader = torch.utils.data.DataLoader(icl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "iwlloader = SamplingLoader(iwldataconfig, dataset=Dataset)\n",
        "iwl_seq_generator = iwlloader.get_seq\n",
        "iwl_dataset = IterDataset(iwl_seq_generator)\n",
        "iwl_dataloader = torch.utils.data.DataLoader(iwl_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "icl2loader = SamplingLoader(icl2dataconfig, dataset=Dataset)\n",
        "icl2_seq_generator = icl2loader.get_seq\n",
        "icl2_dataset = IterDataset(icl2_seq_generator)\n",
        "icl2_dataloader = torch.utils.data.DataLoader(icl2_dataset, batch_size=trainconfig.batch_size, pin_memory=True, num_workers=os.cpu_count())\n",
        "\n",
        "# model\n",
        "embedder = InputEmbedder(config.modelconfig)\n",
        "model = TransformerICL(embedder, config.modelconfig)\n",
        "model.to(config.device)\n",
        "\n",
        "# optimizer\n",
        "if config.trainconfig.optimizer == \"adam\":\n",
        "  optimizer =  torch.optim.Adam(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"sgd\":\n",
        "  optimizer =  torch.optim.SGD(model.parameters(), lr=config.trainconfig.lr)\n",
        "elif config.trainconfig.optimizer == \"adamw\":\n",
        "  optimizer =  torch.optim.AdamW(model.parameters(), lr=config.trainconfig.lr)\n",
        "\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "step = 0\n",
        "for (data_dict, icl_data_dict, iwl_data_dict, icl2_data_dict) in zip(train_dataloader, icl_dataloader, iwl_dataloader, icl2_dataloader):\n",
        "  model.train()   \n",
        "  data_dict = to_gpu_dict(data_dict)\n",
        "  icl_data_dict = to_gpu_dict(icl_data_dict)\n",
        "  iwl_data_dict = to_gpu_dict(iwl_data_dict)\n",
        "  icl2_data_dict = to_gpu_dict(icl2_data_dict)\n",
        "  \n",
        "  logits = model(data_dict[\"examples\"], data_dict[\"labels\"])\n",
        "  query_logit = logits[:,-1,:]\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # print(data_dict[\"labels\"][:,-1])\n",
        "  loss = criterion(query_logit, data_dict[\"labels\"][:,-1],)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  train_acc = cal_acc(data_dict[\"labels\"][:, -1], query_logit)\n",
        "  # print(\"train_sample\", data_dict[\"classes\"], data_dict[\"labels\"])\n",
        "  wandb.log({\"train/acc\":train_acc,\"train/loss\":loss}, step=step)\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    logits = model(icl_data_dict[\"examples\"], icl_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    icl_acc = cal_acc(icl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/icl_acc\":icl_acc}, step=step)\n",
        "    # print(\"icl_sample\", icl_data_dict[\"classes\"], icl_data_dict[\"labels\"])\n",
        "\n",
        "    logits = model(iwl_data_dict[\"examples\"], iwl_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    iwl_acc = cal_acc(iwl_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/iwl_acc\":iwl_acc}, step=step)\n",
        "    # print(\"iwl_sample\", iwl_data_dict[\"classes\"], iwl_data_dict[\"labels\"])\n",
        "\n",
        "    logits = model(icl2_data_dict[\"examples\"], icl2_data_dict[\"labels\"])\n",
        "    query_logit = logits[:,-1,:]\n",
        "    icl2_acc = cal_acc(icl2_data_dict[\"labels\"][:, -1], query_logit)\n",
        "    wandb.log({\"valid/icl2_acc\":icl2_acc}, step=step)\n",
        "    # print(\"icl2_sample\", icl2_data_dict[\"classes\"], icl2_data_dict[\"labels\"])\n",
        "          \n",
        "  print(\"\\r\",step, train_acc.item(), iwl_acc.item(), icl_acc.item(), icl2_acc.item(), end=\"\")\n",
        "  step+=1\n",
        "  if step > config.trainconfig.optimize_step:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4i_bXu6L5yF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "065c4cf26dd24c0d8caaa7165e800ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12621ac3883347eba74b077797cda2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b7930a79e344c358f1ca175e9be4fd1",
              "IPY_MODEL_d4e89f0d98b44eb4b76e59caa06aea14"
            ],
            "layout": "IPY_MODEL_494267ae30c44a3bb446eaee6dc43115"
          }
        },
        "1a6e230516ff4996bb34f8a4992e1a20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "494267ae30c44a3bb446eaee6dc43115": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b7930a79e344c358f1ca175e9be4fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6e230516ff4996bb34f8a4992e1a20",
            "placeholder": "",
            "style": "IPY_MODEL_065c4cf26dd24c0d8caaa7165e800ca7",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "9b3c46b8ff12462b9c4d2f0fbf12c767": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aea29e116b304946b7d98692deb2addb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e89f0d98b44eb4b76e59caa06aea14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aea29e116b304946b7d98692deb2addb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b3c46b8ff12462b9c4d2f0fbf12c767",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
